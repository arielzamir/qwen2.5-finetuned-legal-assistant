{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZxqEVOIHKIVg55BaYb39T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arielzamir/qwen2.5-finetuned-legal-assistant/blob/main/legal_assistant_qwen_finetuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Legal Assistant - Qwen2.5 Fine-Tuned with LoRA"
      ],
      "metadata": {
        "id": "LCdTzQBvOPFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to fine-tune the **Qwen2.5-1.5B-Instruct** model using **LoRA (Low-Rank Adaptation)** on a legal dataset.  \n",
        "We use the Hugging Face ecosystem with `transformers`, `trl`, `peft`, and `datasets`, along with **Weights & Biases (wandb)** for experiment tracking.  \n"
      ],
      "metadata": {
        "id": "vgVM1PPtOmot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install Dependencies"
      ],
      "metadata": {
        "id": "GKCzgNnPO8U3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install all the necessary libraries:\n",
        "- **bitsandbytes** → 8-bit optimizers for efficient training  \n",
        "- **transformers** → Hugging Face model APIs  \n",
        "- **accelerate** → handles multi-GPU / mixed precision training  \n",
        "- **peft** → lightweight fine-tuning with LoRA  \n",
        "- **trl** → supervised fine-tuning (SFT) utilities  \n",
        "- **datasets** → loading and processing datasets"
      ],
      "metadata": {
        "id": "EZ76B-NcQOlW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uecfSEWvpEMe"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U bitsandbytes transformers accelerate peft trl datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries"
      ],
      "metadata": {
        "id": "Xbet6GE2QSbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the core libraries:  \n",
        "- `datasets` → load datasets easily from Hugging Face Hub  \n",
        "- `transformers` → tokenizer + base model  \n",
        "- `trl` → SFTTrainer for fine-tuning  \n",
        "- `peft` → LoRA configs and model wrapping  \n",
        "- `huggingface_hub` → authentication for pushing models  \n",
        "- `wandb` → experiment tracking  \n",
        "- `torch` → PyTorch backend  "
      ],
      "metadata": {
        "id": "p0SQOUQ8QVzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "import torch"
      ],
      "metadata": {
        "id": "iOPWoMA8qMBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Authentication"
      ],
      "metadata": {
        "id": "1Ba6YlvgQrDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we log into:\n",
        "- **Hugging Face Hub** → for downloading models and pushing trained adapters  \n",
        "- **Weights & Biases** → to track metrics, losses, and experiment runs  "
      ],
      "metadata": {
        "id": "f5jsyrJwQs-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "login()\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "1sIvauE4qSgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset Preparation  "
      ],
      "metadata": {
        "id": "wzZ8Mi6aRmma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the **CUAD (Contract Understanding Atticus Dataset)** legal QA dataset.  \n",
        "Each question-answer pair is converted into a **chat format** with roles:  \n",
        "- `system` → defines assistant behavior  \n",
        "- `user` → the question (legal contract query)  \n",
        "- `assistant` → the answer  \n",
        "\n",
        "This ensures the dataset matches the **instruction-tuned format** required by Qwen2.5.  "
      ],
      "metadata": {
        "id": "s2Q3MCiBRqKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_chat(example):\n",
        "    ans = example.get(\"answers\", {}).get(\"text\", [])\n",
        "    answer = ans[0].strip() if len(ans) > 0 else \"\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
        "            {\"role\":\"user\",\"content\": example[\"question\"]},\n",
        "            {\"role\":\"assistant\",\"content\": answer},\n",
        "        ]\n",
        "    }"
      ],
      "metadata": {
        "id": "YUE7tvu7qUsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the Dataset"
      ],
      "metadata": {
        "id": "YIdt-ThCRxgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"chenghao/cuad_qa\")\n",
        "dataset = dataset.map(convert_to_chat, remove_columns=dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "CI0zjv_qqV0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Base Model & Tokenizer  "
      ],
      "metadata": {
        "id": "yFDNHugHR-Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the **Qwen2.5-1.5B-Instruct** model and tokenizer.  \n",
        "- If the tokenizer has no `pad_token`, we assign it to the EOS token.  \n",
        "- The model is loaded in **bfloat16/float16** automatically if GPU supports it.  \n",
        "- Device mapping is set to `\"auto\"` so `accelerate` decides GPU/CPU placement.  "
      ],
      "metadata": {
        "id": "YC_6MFFqSCUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = \"auto\",\n",
        "    attn_implementation=\"sdpa\",\n",
        ")"
      ],
      "metadata": {
        "id": "eh9bM1rsqZhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configure LoRA  "
      ],
      "metadata": {
        "id": "ogYnhgnHBeCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply **LoRA (Low-Rank Adaptation)** for efficient fine-tuning.  \n",
        "Key parameters:  \n",
        "- `r=16` → rank (controls size of LoRA updates)  \n",
        "- `lora_alpha=32` → scaling factor for updates  \n",
        "- `target_modules=[\"q_proj\",\"v_proj\"]` → which layers LoRA adapts  \n",
        "- `lora_dropout=0.05` → regularization  \n",
        "- `bias=\"none\"` → no bias terms are trained  \n",
        "- `task_type=\"CAUSAL_LM\"` → language modeling  \n",
        "\n",
        "This keeps most of the base model **frozen** and trains only small adapter layers.  "
      ],
      "metadata": {
        "id": "9zXigNjpSK9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "j8h_AScvqfSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Configuration\n"
      ],
      "metadata": {
        "id": "8jPeQdhESdH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define the **training arguments** for supervised fine-tuning:  \n",
        "\n",
        "- `output_dir=\"./legal-assistant\"` → where to save checkpoints  \n",
        "- `per_device_train_batch_size=1` → batch size per GPU  \n",
        "- `gradient_accumulation_steps=8` → simulates a larger batch size  \n",
        "- `packing=True` → packs multiple short samples into one sequence for efficiency  \n",
        "- `num_train_epochs=2` → number of full dataset passes  \n",
        "- `learning_rate=1e-4` → initial learning rate  \n",
        "- `lr_scheduler_type=\"cosine\"` → cosine decay schedule  \n",
        "- `warmup_ratio=0.03` → warmup phase for stable training  \n",
        "- `logging_steps=10` → log metrics every 10 steps  \n",
        "- `save_strategy=\"epoch\"` → save checkpoint every epoch  \n",
        "- `fp16=True` → use mixed precision (faster + less memory)  \n",
        "- `gradient_checking=True` → reduce memory usage with checkpointing  \n",
        "- `push_to_hub=True` → push final model to Hugging Face Hub  \n",
        "- `hub_model_id=\"ArielZamir23/legal-assistant-qwen2_5-1_5b-lora\"` → repo name on Hugging Face Hub  \n",
        "- `hub_strategy=\"every_save\"` → push every checkpoint  \n",
        "- `report_to=\"wandb\"` → log training metrics to Weights & Biases  "
      ],
      "metadata": {
        "id": "M1e2dw62Skr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = SFTConfig(\n",
        "    output_dir=\"./legal-assistant\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    packing=True,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"ArielZamir23/legal-assistant-qwen2_5-1_5b-lora\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    report_to=\"wandb\"\n",
        ")"
      ],
      "metadata": {
        "id": "64F9gJhrqhvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialize Weights & Biases (wandb)  "
      ],
      "metadata": {
        "id": "e9e2nSoiSwAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialize a new **wandb run** to track training metrics:  \n",
        "- `project=\"legal-assistant\"` → experiment project name  \n",
        "- `name=\"qwen2.5-1.5b-lora-cuad\"` → specific run name  \n",
        "\n",
        "This lets us monitor:  \n",
        "- Training loss  \n",
        "- Learning rate schedule  \n",
        "- GPU usage and runtime  \n",
        "- Checkpoint saving  "
      ],
      "metadata": {
        "id": "ZgCLl6j5TWu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"legal-assistant\", name=\"qwen2.5-1.5b-lora-cuad\")"
      ],
      "metadata": {
        "id": "ZP_12lnOqift"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Start Training with SFTTrainer  "
      ],
      "metadata": {
        "id": "046c30d-Tdn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create an `SFTTrainer` that will:  \n",
        "- Use our model + tokenizer  \n",
        "- Train on the prepared `cuad_qa` dataset  \n",
        "- Apply the training configuration defined earlier  \n",
        "\n",
        "The trainer handles everything automatically:  \n",
        "- Forward & backward pass  \n",
        "- Optimizer updates  \n",
        "- Loss logging  \n",
        "- Saving checkpoints  \n",
        "- Pushing to Hugging Face Hub  "
      ],
      "metadata": {
        "id": "QKgiPGL7Tg-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    args=training_args\n",
        ")"
      ],
      "metadata": {
        "id": "slrENAk3qlxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ROwmPGK1rFLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference Example (Quick Start)"
      ],
      "metadata": {
        "id": "c0deFBAhW2IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is fine-tuned, we can use it for **legal question answering**.  \n",
        "Below we load the model with `pipeline` from 🤗 Transformers and ask a **domain-specific question**:  \n",
        "\n",
        "**Example Question:**  \n",
        "👉 *\"What is the termination clause in this contract?\"*\n",
        "\n",
        "The model responds with a legally styled answer extracted/generated from the training domain."
      ],
      "metadata": {
        "id": "zG0qkKDoW37U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question = \"What is the termination clause in this contract?\"\n",
        "generator = pipeline(\"text-generation\", model=\"ArielZamir23/legal-assistant-qwen2_5-1_5b-lora\")\n",
        "\n",
        "output = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=256)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "3n2BJz-vT6lq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}